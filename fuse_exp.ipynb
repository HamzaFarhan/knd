{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from collections.abc import Callable\n",
    "\n",
    "import logfire\n",
    "from langfuse import Langfuse\n",
    "from langfuse.client import DatasetItemClient\n",
    "from langfuse.decorators import langfuse_context\n",
    "from langfuse.model import TextPromptClient\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models import KnownModelName\n",
    "\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-7b34804f-78c2-4c5d-8e35-f3b7b9a4123c\"\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-6754a73e-e265-44ae-8ee9-4fe717f8ccb5\"\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = (\n",
    "    f\"Authorization=Basic {base64.b64encode(f'{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}'.encode()).decode()}\"\n",
    ")\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = LANGFUSE_PUBLIC_KEY\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = LANGFUSE_SECRET_KEY\n",
    "\n",
    "logfire.configure(send_to_logfire=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse()\n",
    "DATASET_NAME = \"rfp_evals\"\n",
    "CRITERIA_EXTRACTOR_NAME = \"criteria_extractor\"\n",
    "DEFAULT_MODEL = \"google-gla:gemini-2.0-flash\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Given a datasheet, terms of reference, and position, extract the job requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobRequirements(BaseModel):\n",
    "    education: str\n",
    "    experience: str\n",
    "\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    correct: bool\n",
    "    feedback: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"Feedback for the AI Agent so that it can improve its performance. Only needed if the AI Agent is incorrect.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_dataset_items(items: list[dict], dataset_name: str):\n",
    "    langfuse.create_dataset(name=dataset_name)\n",
    "    for item in items:\n",
    "        langfuse.create_dataset_item(\n",
    "            dataset_name=dataset_name,\n",
    "            input=item[\"input\"],\n",
    "            expected_output=item[\"expected_output\"],\n",
    "            metadata=item[\"metadata\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def dataset_item_client_to_prompt(item: DatasetItemClient) -> str:\n",
    "    return (\n",
    "        f\"Datasheet: {item.input['datasheet']}\\n\"\n",
    "        f\"Terms of Reference: {item.input['tor']}\\n\"\n",
    "        f\"Position: {item.metadata['position']}\\n\"  # type: ignore\n",
    "    )\n",
    "\n",
    "\n",
    "def get_or_create_prompt(\n",
    "    agent_name: str,\n",
    "    prompt_name: str,\n",
    "    prompt: str,\n",
    "    prompt_version: int | None = None,\n",
    "    model: KnownModelName = DEFAULT_MODEL,\n",
    "    labels: list[str] = [\"production\"],\n",
    ") -> TextPromptClient:\n",
    "    try:\n",
    "        return langfuse.get_prompt(name=prompt_name, version=prompt_version, type=\"text\")\n",
    "    except Exception:\n",
    "        created_prompt = langfuse.create_prompt(\n",
    "            name=prompt_name,\n",
    "            type=\"text\",\n",
    "            prompt=prompt,\n",
    "            config={\"model\": model, \"agent_name\": agent_name},\n",
    "            labels=labels,\n",
    "        )\n",
    "        langfuse_context.flush()\n",
    "        langfuse.flush()\n",
    "        return created_prompt\n",
    "\n",
    "\n",
    "async def extract_criteria(\n",
    "    model: KnownModelName, agent_name: str, system_prompt: str, item: DatasetItemClient\n",
    ") -> JobRequirements:\n",
    "    criteria_extractor = Agent(\n",
    "        model=model, name=agent_name, system_prompt=system_prompt, instrument=True, result_type=JobRequirements\n",
    "    )\n",
    "    return (await criteria_extractor.run(dataset_item_client_to_prompt(item))).data\n",
    "\n",
    "\n",
    "async def evaluate_criteria(item: DatasetItemClient, output: JobRequirements) -> Evaluation:\n",
    "    evaluator = Agent(\n",
    "        model=DEFAULT_MODEL,\n",
    "        name=\"criteria_evaluator\",\n",
    "        system_prompt=(\n",
    "            \"Given a datasheet, terms of reference, position, and actual job requirements, \"\n",
    "            \"evaluate if the extracted job requirements by an AI Agent are correct.\\n\"\n",
    "            \"Be ultra strict. You are a strict evaluator. \"\n",
    "            \"Your feedback will be used to improve the model to eventually generate exactly what the expected output is\"\n",
    "        ),\n",
    "        instrument=True,\n",
    "        result_type=Evaluation,\n",
    "    )\n",
    "    user_prompt = dataset_item_client_to_prompt(item) + f\"Extracted Job Requirements: {output.model_dump_json()}\\n\"\n",
    "    evaluation = (await evaluator.run(user_prompt)).data\n",
    "    evaluation.feedback = (\n",
    "        (\n",
    "            f\"{dataset_item_client_to_prompt(item)}\"\n",
    "            f\"Extracted Job Requirements: {output.model_dump_json()}\\n\"\n",
    "            f\"Actual Job Requirements: {item.expected_output}\\n\"\n",
    "            f\"Feedback: {evaluation.feedback}\"\n",
    "        )\n",
    "        if not evaluation.correct\n",
    "        else \"\"\n",
    "    )\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "async def add_criteria_extractor_score(\n",
    "    item: DatasetItemClient, trace_id: str, score_name: str, prompt: TextPromptClient\n",
    "):\n",
    "    output = await extract_criteria(\n",
    "        model=prompt.config[\"model\"],\n",
    "        agent_name=prompt.config[\"agent_name\"],\n",
    "        system_prompt=prompt.prompt.strip(),\n",
    "        item=item,\n",
    "    )\n",
    "    evaluation = await evaluate_criteria(item=item, output=output)\n",
    "    langfuse.score(\n",
    "        trace_id=trace_id,\n",
    "        name=score_name,\n",
    "        value=evaluation.correct,\n",
    "        comment=evaluation.feedback or None,\n",
    "        data_type=\"BOOLEAN\",\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_experiment(\n",
    "    experiment_name: str,\n",
    "    dataset_name: str,\n",
    "    scoring_function: Callable,\n",
    "    agent_name: str,\n",
    "    prompt: str,\n",
    "    prompt_name: str = \"\",\n",
    "    prompt_version: int | None = None,\n",
    "    model: KnownModelName = DEFAULT_MODEL,\n",
    "):\n",
    "    created_prompt = get_or_create_prompt(\n",
    "        agent_name=agent_name,\n",
    "        prompt_name=prompt_name or f\"{agent_name}_system_prompt\",\n",
    "        prompt=prompt,\n",
    "        prompt_version=prompt_version,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    dataset = langfuse.get_dataset(dataset_name)\n",
    "\n",
    "    for item in dataset.items:\n",
    "        with logfire.span(experiment_name) as span:\n",
    "            trace_id = span.get_span_context().trace_id  # type: ignore\n",
    "            trace_id = f\"{trace_id:032x}\"\n",
    "            print(f\"trace_id: {trace_id}\")\n",
    "\n",
    "            with item.observe(\n",
    "                run_name=experiment_name, trace_id=trace_id, run_metadata={\"langfuse_prompt\": created_prompt}\n",
    "            ) as _:\n",
    "                await scoring_function(\n",
    "                    item=item,\n",
    "                    trace_id=trace_id,\n",
    "                    score_name=f\"{agent_name}_evaluation\",\n",
    "                    prompt=created_prompt,\n",
    "                )\n",
    "    langfuse_context.flush()\n",
    "    langfuse.flush()\n",
    "\n",
    "\n",
    "def get_feedback_from_dataset_run(dataset_name: str, experiment_name: str) -> str:\n",
    "    run = langfuse.get_dataset_run(dataset_name=dataset_name, dataset_run_name=experiment_name)\n",
    "    feedback = \"\"\n",
    "    for run_item in run.dataset_run_items:\n",
    "        trace = langfuse.get_trace(run_item.trace_id)\n",
    "        for score in trace.scores:\n",
    "            if not score.value and score.comment:\n",
    "                feedback += f\"Run:\\n{score.comment}\\n---\\n\"\n",
    "    return feedback\n",
    "\n",
    "\n",
    "def update_prompt_with_feedback(\n",
    "    dataset_name: str,\n",
    "    experiment_name: str,\n",
    "    agent_name: str = \"\",\n",
    "    prompt_name: str = \"\",\n",
    "    prompt: str = \"\",\n",
    "    prompt_version: int | None = None,\n",
    "    model: KnownModelName = DEFAULT_MODEL,\n",
    "    labels: list[str] = [\"production\"],\n",
    ") -> TextPromptClient:\n",
    "    feedback = get_feedback_from_dataset_run(dataset_name=dataset_name, experiment_name=experiment_name)\n",
    "    if feedback:\n",
    "        if prompt:\n",
    "            assert agent_name and prompt_name and prompt and prompt_version, (\n",
    "                \"agent_name, prompt_name, prompt, and prompt_version must be provided if prompt is provided\"\n",
    "            )\n",
    "        else:\n",
    "            run = langfuse.get_dataset_run(dataset_name=dataset_name, dataset_run_name=experiment_name)\n",
    "            langfuse_prompt: dict = run.metadata[\"langfuse_prompt\"]  # type: ignore\n",
    "            agent_name = langfuse_prompt[\"config\"][\"agent_name\"]\n",
    "            prompt_name = langfuse_prompt[\"name\"]\n",
    "            prompt = langfuse_prompt[\"prompt\"]\n",
    "            prompt_version = langfuse_prompt[\"version\"] + 1\n",
    "            model = langfuse_prompt[\"config\"].get(\"model\", model)\n",
    "            labels = labels or langfuse_prompt[\"config\"].get(\"labels\", labels)\n",
    "        prompt = prompt.strip() + \"\\n\\nSome feedback from previous runs you got wrong:\\n\" + feedback\n",
    "    return get_or_create_prompt(\n",
    "        agent_name=agent_name,\n",
    "        prompt_name=prompt_name,\n",
    "        prompt=prompt,\n",
    "        prompt_version=prompt_version,\n",
    "        model=model,\n",
    "        labels=labels,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_items = [\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"datasheet\": \"Must have a bachelor's degree in mechanical engineering or a related field.\",\n",
    "            \"tor\": \"Must have done a project for skyscrapers\",\n",
    "        },\n",
    "        \"expected_output\": {\"education\": \"BS in Mechanical Engineering\", \"experience\": \"Skyscraper Project\"},\n",
    "        \"metadata\": {\"position\": \"Mechanical Engineer\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"datasheet\": \"Must have a master's degree in data science or a related field.\",\n",
    "            \"tor\": \"Must have experience with machine learning algorithms\",\n",
    "        },\n",
    "        \"expected_output\": {\"education\": \"MS in Data Science\", \"experience\": \"Machine Learning\"},\n",
    "        \"metadata\": {\"position\": \"Data Scientist\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"datasheet\": \"Must have a PhD in physics or a related field.\",\n",
    "            \"tor\": \"Must have published research in quantum mechanics\",\n",
    "        },\n",
    "        \"expected_output\": {\"education\": \"PhD in Physics\", \"experience\": \"Quantum Mechanics Research\"},\n",
    "        \"metadata\": {\"position\": \"Quantum Physicist\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "system_prompt = \"Given a datasheet, terms of reference, and position, extract the job requirements.\\n\"\n",
    "\n",
    "messy_system_prompt = (\n",
    "    system_prompt\n",
    "    + \"(mess up the output a bit (like MS instead of BS). i wanna see if the evaluator catches it).\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dataset_items(items=dataset_items, dataset_name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_name = f\"{CRITERIA_EXTRACTOR_NAME}_system_prompt\"\n",
    "\n",
    "experiment_number = 1\n",
    "experiment_name = f\"{CRITERIA_EXTRACTOR_NAME}_run_{experiment_number}\"\n",
    "prompt_version = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_system_prompt = get_or_create_prompt(\n",
    "    agent_name=CRITERIA_EXTRACTOR_NAME,\n",
    "    prompt_name=prompt_name,\n",
    "    prompt=messy_system_prompt,\n",
    "    prompt_version=prompt_version,\n",
    "    model=DEFAULT_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(created_system_prompt.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    scoring_function=add_criteria_extractor_score,\n",
    "    agent_name=CRITERIA_EXTRACTOR_NAME,\n",
    "    prompt_name=prompt_name,\n",
    "    prompt=messy_system_prompt,\n",
    "    prompt_version=prompt_version,\n",
    "    model=DEFAULT_MODEL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_system_prompt = update_prompt_with_feedback(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    experiment_name=experiment_name,\n",
    "    agent_name=CRITERIA_EXTRACTOR_NAME,\n",
    "    prompt=system_prompt,\n",
    "    prompt_name=prompt_name,\n",
    "    prompt_version=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated_system_prompt.prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = get_feedback_from_dataset_run(\n",
    "    dataset_name=DATASET_NAME, experiment_name=f\"{CRITERIA_EXTRACTOR_NAME}_run_1\"\n",
    ")\n",
    "print(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
